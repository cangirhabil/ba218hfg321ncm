import datetime
import json
import os
import time
from os.path import exists
from typing import List
from urllib.parse import urlencode

import litellm
from jinja2 import Environment, StrictUndefined

from HTTPRequest import HTTPRequest
from general_functions import URL

import sys
rootpath = os.path.abspath('..')
print(rootpath)
sys.path.insert(1,rootpath+'/settings')
from config_loader import get_settings



class AICaller:
    def __init__(self, model: str, api_base: str = ""):
        """
        Initializes an instance of the AICaller class.

        Parameters:
            model (str): The name of the model to be used.
            api_base (str): The base API url to use in case model is set to Ollama or Hugging Face
        """
        self.model = model
        self.api_base = api_base

    def call_model(self, prompt: dict, max_tokens=4096):
        """
        Call the language model with the provided prompt and retrieve the response.

        Parameters:
            prompt (dict): The prompt to be sent to the language model.
            max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 4096.

        Returns:
            tuple: A tuple containing the response generated by the language model, the number of tokens used from the prompt, and the total number of tokens in the response.
        """
        if "system" not in prompt or "user" not in prompt:
            raise KeyError(
                "The prompt dictionary must contain 'system' and 'user' keys."
            )
        if prompt["system"] == "":
            messages = [{"role": "user", "content": prompt["user"]}]
        else:
            messages = [
                {"role": "system", "content": prompt["system"]},
                {"role": "user", "content": prompt["user"]},
            ]

        # Default Completion parameters
        completion_params = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "stream": True,
            "temperature": 0.2,
            ## ARYA EDIT, for Gemini
            # "response_format": {"type": "json_object"}
        }

        # API base exception for OpenAI Compatible, Ollama and Hugging Face models
        if (
                "ollama" in self.model
                or "huggingface" in self.model
                or self.model.startswith("openai/")
        ):
            completion_params["api_base"] = self.api_base

        response = litellm.completion(**completion_params)

        chunks = []
        print("Streaming results from LLM model...")
        try:
            for chunk in response:
                print(chunk.choices[0].delta.content or "", end="", flush=True)
                chunks.append(chunk)
                time.sleep(
                    0.01
                )  # Optional: Delay to simulate more 'natural' response pacing
        except Exception as e:
            print(f"Error during streaming: {e}")
        print("\n")


        model_response = litellm.stream_chunk_builder(chunks, messages=messages)

        # Returns: Response, Prompt token count, and Response token count
        return (
            model_response["choices"][0]["message"]["content"],
            int(model_response["usage"]["prompt_tokens"]),
            int(model_response["usage"]["completion_tokens"]),
        )

def build_prompt(endpoint: URL, payload: str) -> dict:
    variables = {
        "endpoint": endpoint,
        "payload": payload
    }
    environment = Environment(undefined=StrictUndefined)
    try:
        system_prompt = environment.from_string(
            get_settings().payload_prompt.system
        ).render(variables)
        user_prompt = environment.from_string(
            get_settings().payload_prompt.user
        ).render(variables)
    except Exception as e:
        print(e)
        return {"system": "", "user": ""}

    return {"system": system_prompt, "user": user_prompt}

def build_expand_prompt(request_list: List[HTTPRequest]) -> dict:
    endpoint = ""
    for req in request_list:
        endpoint += f"{req.method} {req.full_url} {req.post_data_encoded}\n"

    variables = {
        "endpoint": endpoint
    }
    environment = Environment(undefined=StrictUndefined)
    try:
        system_prompt = environment.from_string(
            get_settings().expand_payload_prompt.system
        ).render(variables)
        user_prompt = environment.from_string(
            get_settings().expand_payload_prompt.user
        ).render(variables)
    except Exception as e:
        print(e)
        return {"system": "", "user": ""}

    return {"system": system_prompt, "user": user_prompt}

def build_general_prompt(endpoint: URL, payload: str) -> dict:
    variables = {
        "endpoint": endpoint,
        "payload": payload
    }
    environment = Environment(undefined=StrictUndefined)
    try:
        system_prompt = environment.from_string(
            get_settings().general_prompt.system
        ).render(variables)
        user_prompt = environment.from_string(
            get_settings().general_prompt.user
        ).render(variables)
    except Exception as e:
        print(e)
        return {"system": "", "user": ""}

    return {"system": system_prompt, "user": user_prompt}

def is_encoded_payload(txt: str):
    if txt.find('=')>0 and txt.find('&')>0:
        return True
    elif txt.find('=')>0:
        return True
    return False

def is_json_payload(txt: str):
    if txt.find('{')>-1 and txt.find(':')>-1 and txt.find('}')>-1:
        return True
    return False

def cleaning_from_asterisk(txt: str):
    while len(txt)>2 and txt[-1]=="*":
        txt = txt[:-1]

    while len(txt)>2 and txt[0]=="*":
        txt = txt[1:]

    return txt

def parse_response(response: str):
    print(f"[AICALLER] Parsing the LLM response")
    payloads = list()
    urls = list()
    for txt_raw in response.split():
        # print("Parsing:", txt)
        txt = cleaning_from_asterisk(txt_raw)
        if txt.find("/")==0 or txt.find("http")==0:
            urls.append(txt)
        elif is_encoded_payload(txt):
            payloads.append(txt)
        elif is_json_payload(txt):
            try:
                data = json.loads(txt)
                payloads.append(urlencode(data, doseq=True))
            except Exception as e:
                print(e)
    # print("Payload from LLM:",payload)
    return payloads, urls

def build_param_filtering_prompt(request: HTTPRequest) -> dict:
    variables = {
        "method": request.method,
        "full_url": request.full_url,
        "body_payload": request.post_data_encoded
    }
    environment = Environment(undefined=StrictUndefined)
    try:
        system_prompt = environment.from_string(
            get_settings().param_filtering_prompt.system
        ).render(variables)
        user_prompt = environment.from_string(
            get_settings().param_filtering_prompt.user
        ).render(variables)
    except Exception as e:
        print(e)
        return {"system": "", "user": ""}

    return {"system": system_prompt, "user": user_prompt}

def build_generation_prompt(request_list: List[HTTPRequest], response_titles) -> dict:
    endpoint = ""
    for req in request_list:
        param_vals = [pv.get_formatted_paramval() for pv in req.get_all_atomic_param_vals()]
        endpoint += f"{req.method} {req.full_url} {param_vals}\n"

    results = ""
    for res in response_titles:
        if res and len(res)>0:
            results += f"{res}\n"

    if len(results)==0:
        results = "Sorry, I do not try the request yet, so we do not have any response"

    variables = {
        "role": request_list[0].role,
        "endpoint": endpoint,
        "results": results
    }
    environment = Environment(undefined=StrictUndefined)
    try:
        system_prompt = environment.from_string(
            get_settings().payload_generation_prompt.system
        ).render(variables)
        user_prompt = environment.from_string(
            get_settings().payload_generation_prompt.user
        ).render(variables)
    except Exception as e:
        print(e)
        return {"system": "", "user": ""}

    return {"system": system_prompt, "user": user_prompt}
